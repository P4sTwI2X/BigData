{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from shapely.geometry import Point, Polygon\n",
    "import pyspark.sql.utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this parameter accordingly to your machine\n",
    "FILE_DIR = \"file:///home/p4stwi2x/Desktop/abs/taxi-data/\"\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    "          .appName(\"RegionEventCount\")\\\n",
    "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvSchema = StructType([StructField(\"type\", StringType(), True),\n",
    "                        StructField(\"VendorID\", IntegerType(), True),\n",
    "                        StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "                        StructField(\"tpep_dropoff_datetime\",TimestampType(), True),\n",
    "\n",
    "                        StructField(\"blankCol1\", StringType(), True),\n",
    "                        StructField(\"blankCol2\", StringType(), True),\n",
    "                        StructField(\"blankCol3\", StringType(), True),\n",
    "                        StructField(\"blankCol4\", StringType(), True),\n",
    "\n",
    "                        StructField(\"long_green\",DoubleType(), True),\n",
    "                        StructField(\"lat_green\", DoubleType(), True),\n",
    "                        StructField(\"long_yellow\", DoubleType(), True),\n",
    "                        StructField(\"lat_yellow\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- blankCol1: string (nullable = true)\n",
      " |-- blankCol2: string (nullable = true)\n",
      " |-- blankCol3: string (nullable = true)\n",
      " |-- blankCol4: string (nullable = true)\n",
      " |-- long_green: double (nullable = true)\n",
      " |-- lat_green: double (nullable = true)\n",
      " |-- long_yellow: double (nullable = true)\n",
      " |-- lat_yellow: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamingInputDF_ex03 = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .schema(csvSchema)\n",
    "    # .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(FILE_DIR)\n",
    ")\n",
    "\n",
    "df_with_lat_long = streamingInputDF_ex03.withColumn(\n",
    "    \"lat\",\n",
    "    when(col(\"type\") == \"green\", col(\"lat_green\"))\n",
    "    .when(col(\"type\") == \"yellow\", col(\"lat_yellow\"))\n",
    ").withColumn(\n",
    "    \"long\",\n",
    "    when(col(\"type\") == \"green\", col(\"long_green\"))\n",
    "    .when(col(\"type\") == \"yellow\", col(\"long_yellow\"))\n",
    ")\n",
    "\n",
    "df_with_lat_long.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_HQ(long, lat)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldman = [[-74.0141012, 40.7152191], [-74.013777, 40.7152275], [-74.0141027, 40.7138745], [-74.0144185, 40.7140753]]\n",
    "citigroup = [[-74.011869, 40.7217236], [-74.009867, 40.721493], [-74.010140,40.720053], [-74.012083, 40.720267]]\n",
    "def get_HQ(long, lat):\n",
    "    pt = Point(long, lat)\n",
    "    gd_p = Polygon(goldman)\n",
    "    ct_p = Polygon(citigroup)\n",
    "\n",
    "    \n",
    "    if gd_p.contains(pt):\n",
    "        return \"goldman\"\n",
    "    elif ct_p.contains(pt):\n",
    "        return \"citigroup\"\n",
    "    return \"unknown\"\n",
    "\n",
    "spark.udf.register(\"getHQ\", get_HQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- blankCol1: string (nullable = true)\n",
      " |-- blankCol2: string (nullable = true)\n",
      " |-- blankCol3: string (nullable = true)\n",
      " |-- blankCol4: string (nullable = true)\n",
      " |-- long_green: double (nullable = true)\n",
      " |-- lat_green: double (nullable = true)\n",
      " |-- long_yellow: double (nullable = true)\n",
      " |-- lat_yellow: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- drop_loc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_lat_long = df_with_lat_long\\\n",
    "    .withColumn(\"drop_loc\", expr(\"getHQ(long, lat)\"))\\\n",
    "    .where(col(\"drop_loc\") != \"unknown\")\n",
    "\n",
    "df_with_lat_long.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingCountsDF = (\n",
    "  df_with_lat_long.select(\"drop_loc\", \"tpep_dropoff_datetime\")\n",
    "    .groupBy(\n",
    "        df_with_lat_long.drop_loc,\n",
    "        window(df_with_lat_long.tpep_dropoff_datetime, \"10 minutes\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")         # console or memory(= store in-memory table)\n",
    "    .queryName(\"counts\")      # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")\n",
    "    # .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.processAllAvailable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+-----+\n",
      "|drop_loc |window                                    |count|\n",
      "+---------+------------------------------------------+-----+\n",
      "|citigroup|{2015-12-01 00:10:00, 2015-12-01 00:20:00}|1    |\n",
      "|citigroup|{2015-12-01 00:20:00, 2015-12-01 00:30:00}|2    |\n",
      "|citigroup|{2015-12-01 00:50:00, 2015-12-01 01:00:00}|2    |\n",
      "|citigroup|{2015-12-01 01:00:00, 2015-12-01 01:10:00}|1    |\n",
      "|citigroup|{2015-12-01 01:40:00, 2015-12-01 01:50:00}|1    |\n",
      "|citigroup|{2015-12-01 02:40:00, 2015-12-01 02:50:00}|1    |\n",
      "|citigroup|{2015-12-01 04:00:00, 2015-12-01 04:10:00}|1    |\n",
      "|citigroup|{2015-12-01 05:10:00, 2015-12-01 05:20:00}|1    |\n",
      "|goldman  |{2015-12-01 05:20:00, 2015-12-01 05:30:00}|1    |\n",
      "|citigroup|{2015-12-01 05:40:00, 2015-12-01 05:50:00}|1    |\n",
      "|goldman  |{2015-12-01 05:50:00, 2015-12-01 06:00:00}|2    |\n",
      "|citigroup|{2015-12-01 05:50:00, 2015-12-01 06:00:00}|6    |\n",
      "|goldman  |{2015-12-01 06:00:00, 2015-12-01 06:10:00}|1    |\n",
      "|goldman  |{2015-12-01 06:10:00, 2015-12-01 06:20:00}|1    |\n",
      "|citigroup|{2015-12-01 06:10:00, 2015-12-01 06:20:00}|1    |\n",
      "|goldman  |{2015-12-01 06:20:00, 2015-12-01 06:30:00}|1    |\n",
      "|citigroup|{2015-12-01 06:20:00, 2015-12-01 06:30:00}|7    |\n",
      "|citigroup|{2015-12-01 06:30:00, 2015-12-01 06:40:00}|7    |\n",
      "|goldman  |{2015-12-01 06:30:00, 2015-12-01 06:40:00}|2    |\n",
      "|goldman  |{2015-12-01 06:40:00, 2015-12-01 06:50:00}|3    |\n",
      "+---------+------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Query executed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql('select drop_loc , window, count from counts order by window').show(truncate=False)\n",
    "    print (\"Query executed\")\n",
    "except pyspark.sql.utils.AnalysisException:\n",
    "    print(\"Unable to process your query!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+-----+----+\n",
      "|drop_loc |window                                    |count|prev|\n",
      "+---------+------------------------------------------+-----+----+\n",
      "|citigroup|{2015-12-01 08:50:00, 2015-12-01 09:00:00}|12   |3   |\n",
      "|citigroup|{2015-12-01 14:00:00, 2015-12-01 14:10:00}|10   |3   |\n",
      "+---------+------------------------------------------+-----+----+\n",
      "\n",
      "Query executed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql('SELECT drop_loc, window, count, prev FROM (SELECT  drop_loc, window, count, lag(count, 1, NULL) OVER (ORDER BY drop_loc, window) AS prev FROM counts) WHERE count >= 10 and count >= 2 * prev AND prev IS NOT NULL').show(100,truncate=False)\n",
    "    print (\"Query executed\")\n",
    "except pyspark.sql.utils.AnalysisException:\n",
    "    print(\"Unable to process your query!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+-----+----+\n",
      "|drop_loc |window                                    |count|prev|\n",
      "+---------+------------------------------------------+-----+----+\n",
      "|citigroup|{2015-12-01 06:40:00, 2015-12-01 06:50:00}|13   |NULL|\n",
      "|citigroup|{2015-12-01 06:50:00, 2015-12-01 07:00:00}|18   |13  |\n",
      "|citigroup|{2015-12-01 07:00:00, 2015-12-01 07:10:00}|15   |18  |\n",
      "|citigroup|{2015-12-01 07:10:00, 2015-12-01 07:20:00}|10   |15  |\n",
      "|citigroup|{2015-12-01 07:20:00, 2015-12-01 07:30:00}|10   |10  |\n",
      "|citigroup|{2015-12-01 07:40:00, 2015-12-01 07:50:00}|10   |10  |\n",
      "|citigroup|{2015-12-01 08:00:00, 2015-12-01 08:10:00}|11   |10  |\n",
      "|citigroup|{2015-12-01 08:20:00, 2015-12-01 08:30:00}|14   |11  |\n",
      "|citigroup|{2015-12-01 08:50:00, 2015-12-01 09:00:00}|12   |14  |\n",
      "|citigroup|{2015-12-01 09:00:00, 2015-12-01 09:10:00}|16   |12  |\n",
      "|goldman  |{2015-12-01 09:20:00, 2015-12-01 09:30:00}|11   |16  |\n",
      "|citigroup|{2015-12-01 09:30:00, 2015-12-01 09:40:00}|12   |11  |\n",
      "|goldman  |{2015-12-01 09:30:00, 2015-12-01 09:40:00}|10   |12  |\n",
      "|citigroup|{2015-12-01 09:50:00, 2015-12-01 10:00:00}|10   |10  |\n",
      "|citigroup|{2015-12-01 14:00:00, 2015-12-01 14:10:00}|10   |10  |\n",
      "+---------+------------------------------------------+-----+----+\n",
      "\n",
      "Query executed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql('SELECT drop_loc, window, count, prev FROM (SELECT  drop_loc, window, count, lag(count, 1, NULL) OVER (ORDER BY window) AS prev FROM counts WHERE count >= 10)').show(100, truncate=False)\n",
    "    print (\"Query executed\")\n",
    "except pyspark.sql.utils.AnalysisException:\n",
    "    print(\"Unable to process your query!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrivals to Citigroup has doubled from 3 to 12 at Row(start=datetime.datetime(2015, 12, 1, 8, 50), end=datetime.datetime(2015, 12, 1, 9, 0))!\n",
      "Timestamp 360000 has been exported to folder file:///home/p4stwi2x/Desktop/abs/output_task_4/output-360000\n",
      "The number of arrivals to Citigroup has doubled from 3 to 10 at Row(start=datetime.datetime(2015, 12, 1, 14, 0), end=datetime.datetime(2015, 12, 1, 14, 10))!\n",
      "Timestamp 60000 has been exported to folder file:///home/p4stwi2x/Desktop/abs/output_task_4/output-60000\n",
      "Number of arrivals to Goldman Sachs : 59\n",
      "Number of arrivals to Citigroup : 111\n"
     ]
    }
   ],
   "source": [
    "output_path_ex04 = 'file:///home/p4stwi2x/Desktop/abs/output_task_4'\n",
    "\n",
    "trending = spark.sql('SELECT drop_loc, window, count, prev FROM (SELECT  drop_loc, window, count, lag(count, 1, NULL) OVER (ORDER BY drop_loc, window) AS prev FROM counts) WHERE count >= 10 and count >= 2 * prev AND prev IS NOT NULL')\\\n",
    "          .withColumn(\"temp\", (minute('window.start')+10) * 6000)\n",
    "\n",
    "minute_count = spark.sql('SELECT * from counts ORDER BY window')\\\n",
    "          .withColumn(\"temp\", (minute('window.start')+10)*60000)\n",
    "\n",
    "for trend in trending.collect():\n",
    "    trend_timestamp_count = trend['temp']\n",
    "    trend_headquarter = trend['drop_loc']\n",
    "    trend_windows_count = trend['count']\n",
    "    trend_windows_data = trend['window']\n",
    "    prev_count = trend['prev']\n",
    "\n",
    "    headquarter = \"Goldman Sachs\" if (trend_headquarter == \"goldman\") else \"Citigroup\"\n",
    "    print(f\"The number of arrivals to {headquarter} has doubled from {prev_count} to {trend_windows_count} at {trend_windows_data}!\")\n",
    "\n",
    "    output_dir = os.path.join(output_path_ex04, f\"output-{trend_timestamp_count}\")\n",
    "\n",
    "    df_windows = spark.createDataFrame([(trend_headquarter,trend_windows_count,trend_timestamp_count,prev_count)], [\"headquarter\",\"current_value\",\"timestamp\",\"prev_value\"])\n",
    "\n",
    "    df_windows.write.mode(\"overwrite\")\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .save(output_dir)\n",
    "\n",
    "    print(f\"Timestamp {trend_timestamp_count} has been exported to folder {output_dir}\")\n",
    "\n",
    "log_path = os.path.join(output_path_ex04, \"output.log\")\n",
    "\n",
    "if not os.path.exists(output_path_ex04):\n",
    "    os.makedirs(output_path_ex04)\n",
    "if not os.path.exists(log_path):\n",
    "  with open(log_path, 'a') as file:  # /content/drive/MyDrive/data/output_task_4/output.log'\n",
    "      file.write(f\"Number of arrivals to Goldman Sachs : \" + str(minute_count.filter(minute_count.drop_loc == \"goldman\").count()) + \"\\n\")\n",
    "      file.write(f\"Number of arrivals to Citigroup : \" + str(minute_count.filter(minute_count.drop_loc == \"citigroup\").count()))\n",
    "\n",
    "print(f\"Number of arrivals to Goldman Sachs : \" + str(minute_count.filter(minute_count.drop_loc == \"goldman\").count()))\n",
    "print(f\"Number of arrivals to Citigroup : \" + str(minute_count.filter(minute_count.drop_loc == \"citigroup\").count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
