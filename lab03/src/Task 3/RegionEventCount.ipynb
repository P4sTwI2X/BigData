{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from shapely.geometry import Point, Polygon\n",
    "import pyspark.sql.utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this parameter accordingly to your machine\n",
    "FILE_DIR = \"file:///home/p4stwi2x/Desktop/abs/taxi-data/\"\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    "          .appName(\"RegionEventCount\")\\\n",
    "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvSchema = StructType([StructField(\"type\", StringType(), True),\n",
    "                        StructField(\"VendorID\", IntegerType(), True),\n",
    "                        StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "                        StructField(\"tpep_dropoff_datetime\",TimestampType(), True),\n",
    "\n",
    "                        StructField(\"blankCol1\", StringType(), True),\n",
    "                        StructField(\"blankCol2\", StringType(), True),\n",
    "                        StructField(\"blankCol3\", StringType(), True),\n",
    "                        StructField(\"blankCol4\", StringType(), True),\n",
    "\n",
    "                        StructField(\"long_green\",DoubleType(), True),\n",
    "                        StructField(\"lat_green\", DoubleType(), True),\n",
    "                        StructField(\"long_yellow\", DoubleType(), True),\n",
    "                        StructField(\"lat_yellow\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- blankCol1: string (nullable = true)\n",
      " |-- blankCol2: string (nullable = true)\n",
      " |-- blankCol3: string (nullable = true)\n",
      " |-- blankCol4: string (nullable = true)\n",
      " |-- long_green: double (nullable = true)\n",
      " |-- lat_green: double (nullable = true)\n",
      " |-- long_yellow: double (nullable = true)\n",
      " |-- lat_yellow: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamingInputDF_ex03 = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .schema(csvSchema)\n",
    "    # .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(FILE_DIR)\n",
    ")\n",
    "\n",
    "df_with_lat_long = streamingInputDF_ex03.withColumn(\n",
    "    \"lat\",\n",
    "    when(col(\"type\") == \"green\", col(\"lat_green\"))\n",
    "    .when(col(\"type\") == \"yellow\", col(\"lat_yellow\"))\n",
    ").withColumn(\n",
    "    \"long\",\n",
    "    when(col(\"type\") == \"green\", col(\"long_green\"))\n",
    "    .when(col(\"type\") == \"yellow\", col(\"long_yellow\"))\n",
    ")\n",
    "\n",
    "df_with_lat_long.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_HQ(long, lat)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldman = [[-74.0141012, 40.7152191], [-74.013777, 40.7152275], [-74.0141027, 40.7138745], [-74.0144185, 40.7140753]]\n",
    "citigroup = [[-74.011869, 40.7217236], [-74.009867, 40.721493], [-74.010140,40.720053], [-74.012083, 40.720267]]\n",
    "def get_HQ(long, lat):\n",
    "    pt = Point(long, lat)\n",
    "    gd_p = Polygon(goldman)\n",
    "    ct_p = Polygon(citigroup)\n",
    "\n",
    "    \n",
    "    if gd_p.contains(pt):\n",
    "        return \"goldman\"\n",
    "    elif ct_p.contains(pt):\n",
    "        return \"citigroup\"\n",
    "    return \"unknown\"\n",
    "\n",
    "spark.udf.register(\"getHQ\", get_HQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- blankCol1: string (nullable = true)\n",
      " |-- blankCol2: string (nullable = true)\n",
      " |-- blankCol3: string (nullable = true)\n",
      " |-- blankCol4: string (nullable = true)\n",
      " |-- long_green: double (nullable = true)\n",
      " |-- lat_green: double (nullable = true)\n",
      " |-- long_yellow: double (nullable = true)\n",
      " |-- lat_yellow: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- drop_loc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_lat_long = df_with_lat_long\\\n",
    "    .withColumn(\"drop_loc\", expr(\"getHQ(long, lat)\"))\\\n",
    "    .where(col(\"drop_loc\") != \"unknown\")\n",
    "\n",
    "df_with_lat_long.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingCountsDF = (\n",
    "  df_with_lat_long.select(\"drop_loc\", \"tpep_dropoff_datetime\")\n",
    "    .groupBy(\n",
    "        df_with_lat_long.drop_loc,\n",
    "        window(df_with_lat_long.tpep_dropoff_datetime, \"1 hour\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")         # console or memory(= store in-memory table)\n",
    "    .queryName(\"counts\")      # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")\n",
    "    # .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.processAllAvailable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+-----+\n",
      "|drop_loc |window                                    |count|\n",
      "+---------+------------------------------------------+-----+\n",
      "|citigroup|{2015-12-01 00:00:00, 2015-12-01 01:00:00}|5    |\n",
      "|citigroup|{2015-12-01 01:00:00, 2015-12-01 02:00:00}|2    |\n",
      "|citigroup|{2015-12-01 02:00:00, 2015-12-01 03:00:00}|1    |\n",
      "|citigroup|{2015-12-01 04:00:00, 2015-12-01 05:00:00}|1    |\n",
      "|citigroup|{2015-12-01 05:00:00, 2015-12-01 06:00:00}|8    |\n",
      "|goldman  |{2015-12-01 05:00:00, 2015-12-01 06:00:00}|3    |\n",
      "|goldman  |{2015-12-01 06:00:00, 2015-12-01 07:00:00}|11   |\n",
      "|citigroup|{2015-12-01 06:00:00, 2015-12-01 07:00:00}|46   |\n",
      "|goldman  |{2015-12-01 07:00:00, 2015-12-01 08:00:00}|17   |\n",
      "|citigroup|{2015-12-01 07:00:00, 2015-12-01 08:00:00}|62   |\n",
      "|goldman  |{2015-12-01 08:00:00, 2015-12-01 09:00:00}|25   |\n",
      "|citigroup|{2015-12-01 08:00:00, 2015-12-01 09:00:00}|56   |\n",
      "|citigroup|{2015-12-01 09:00:00, 2015-12-01 10:00:00}|60   |\n",
      "|goldman  |{2015-12-01 09:00:00, 2015-12-01 10:00:00}|39   |\n",
      "|citigroup|{2015-12-01 10:00:00, 2015-12-01 11:00:00}|18   |\n",
      "|goldman  |{2015-12-01 10:00:00, 2015-12-01 11:00:00}|26   |\n",
      "|goldman  |{2015-12-01 11:00:00, 2015-12-01 12:00:00}|16   |\n",
      "|citigroup|{2015-12-01 11:00:00, 2015-12-01 12:00:00}|17   |\n",
      "|citigroup|{2015-12-01 12:00:00, 2015-12-01 13:00:00}|24   |\n",
      "|goldman  |{2015-12-01 12:00:00, 2015-12-01 13:00:00}|7    |\n",
      "+---------+------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Query executed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\"select * from counts order by window\").show(truncate=False)\n",
    "    print (\"Query executed\")\n",
    "except pyspark.sql.utils.AnalysisException:\n",
    "    print(\"Unable to process your query!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp 360000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-360000\n",
      "Timestamp 720000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-720000\n",
      "Timestamp 1080000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-1080000\n",
      "Timestamp 1800000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-1800000\n",
      "Timestamp 2160000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-2160000\n",
      "Timestamp 2160000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-2160000\n",
      "Timestamp 2520000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-2520000\n",
      "Timestamp 2520000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-2520000\n",
      "Timestamp 2880000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-2880000\n",
      "Timestamp 2880000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-2880000\n",
      "Timestamp 3240000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-3240000\n",
      "Timestamp 3240000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-3240000\n",
      "Timestamp 3600000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-3600000\n",
      "Timestamp 3600000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-3600000\n",
      "Timestamp 3960000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-3960000\n",
      "Timestamp 3960000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-3960000\n",
      "Timestamp 4320000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-4320000\n",
      "Timestamp 4320000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-4320000\n",
      "Timestamp 4680000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-4680000\n",
      "Timestamp 4680000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-4680000\n",
      "Timestamp 5040000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-5040000\n",
      "Timestamp 5040000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-5040000\n",
      "Timestamp 5400000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-5400000\n",
      "Timestamp 5400000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-5400000\n",
      "Timestamp 5760000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-5760000\n",
      "Timestamp 5760000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-5760000\n",
      "Timestamp 6120000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-6120000\n",
      "Timestamp 6120000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-6120000\n",
      "Timestamp 6480000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-6480000\n",
      "Timestamp 6480000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-6480000\n",
      "Timestamp 6840000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-6840000\n",
      "Timestamp 7200000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-7200000\n",
      "Timestamp 7200000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-7200000\n",
      "Timestamp 7560000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-7560000\n",
      "Timestamp 7560000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-7560000\n",
      "Timestamp 7920000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-7920000\n",
      "Timestamp 8280000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-8280000\n",
      "Timestamp 8640000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-8640000\n",
      "Timestamp 8640000 has been appended to folder file:///home/p4stwi2x/Desktop/abs/output_task_3/output-8640000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "output_path_ex03 = 'file:///home/p4stwi2x/Desktop/abs/output_task_3'\n",
    "hour_count = spark.sql('select * from counts order by window').withColumn(\"temp\", (hour('window.start')+1)*360000)\n",
    "for hour in hour_count.collect():\n",
    "    timestamp_count = hour['temp']\n",
    "    windows_data = hour['drop_loc']\n",
    "    windows_count = hour['count']\n",
    "    output_dir = os.path.join(output_path_ex03, f\"output-{timestamp_count}\")\n",
    "\n",
    "    # Create a DataFrame for the current hour's data\n",
    "    df_windows = spark.createDataFrame([(windows_data, windows_count,)], [\"headquarter\",\"count\"])\n",
    "\n",
    "    \"\"\"if not os.path.exists(output_dir):\n",
    "        # If directory does not exist, write the new DataFrame\n",
    "        df_windows.write.mode(\"overwrite\")\\\n",
    "                .format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .save(output_dir)\n",
    "        print(f\"Timestamp {timestamp_count} has been exported to folder {output_dir}\")\n",
    "    else:\"\"\"\n",
    "        # If directory exists, read and write to the existing CSV\n",
    "    df_windows.write.mode(\"append\")\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .save(output_dir)\n",
    "    print(f\"Timestamp {timestamp_count} has been appended to folder {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
